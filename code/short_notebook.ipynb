{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1672bf14-a444-4ff6-b105-9b53299daaeb",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine learning project, fall 2021 (short notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a0a7d-02e0-464a-a268-406eaafb82cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d17fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from flaml import AutoML\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import moscow_housing_utils as mhu\n",
    "\n",
    "SEED = 123 # for reproducibility\n",
    "np.random.seed(SEED)\n",
    "sns.set_style('darkgrid')\n",
    "pd.set_option('display.max_colwidth', None) # don't truncate cell width\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7995c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and merge data\n",
    "apartments = pd.read_csv('../data/apartments_train.csv')\n",
    "buildings = pd.read_csv('../data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('../data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('../data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "309c3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrangling_optimal(all_data):\n",
    "    \"\"\"\n",
    "    Perform the best data wrangling we know (for LGBM) on all data we have\n",
    "    \"\"\"\n",
    "    X = all_data.copy()\n",
    "    \n",
    "    # impute seller (negative value is interpreted as missing by LightGBM)\n",
    "    X['seller'] = X['seller'].fillna(-1.0)\n",
    "    \n",
    "    X = mhu.delete_suspicious_data(X)\n",
    "\n",
    "    # impute areas\n",
    "    quants = X.area_total.quantile([0.25, 0.5, 0.75])\n",
    "    (q1, q2, q3) = (quants.iloc[0], quants.iloc[1], quants.iloc[2])\n",
    "\n",
    "    X.loc[X.area_kitchen.isnull() & (X.area_total <= q1), 'area_kitchen'] = \\\n",
    "        X.loc[X.area_total <= q1, 'area_kitchen'].median()\n",
    "    X.loc[X.area_kitchen.isnull() & (q1 < X.area_total) & (X.area_total\n",
    "          <= q2), 'area_kitchen'] = X.loc[(q1 < X.area_total)\n",
    "            & (X.area_total <= q2), 'area_kitchen'].median()\n",
    "    X.loc[X.area_kitchen.isnull() & (q2 < X.area_total) & (X.area_total\n",
    "          <= q3), 'area_kitchen'] = X.loc[(q2 < X.area_total)\n",
    "            & (X.area_total <= q3), 'area_kitchen'].median()\n",
    "    X.loc[X.area_kitchen.isnull() & (q3 < X.area_total), 'area_kitchen'] = \\\n",
    "        X.loc[q3 < X.area_total, 'area_kitchen'].median()\n",
    "\n",
    "    X.loc[X.area_living.isnull() & (X.area_total <= q1), 'area_living'] = \\\n",
    "        X.loc[X.area_total <= q1, 'area_living'].median()\n",
    "    X.loc[X.area_living.isnull() & (q1 < X.area_total) & (X.area_total\n",
    "          <= q2), 'area_living'] = X.loc[(q1 < X.area_total)\n",
    "            & (X.area_total <= q2), 'area_living'].median()\n",
    "    X.loc[X.area_living.isnull() & (q2 < X.area_total) & (X.area_total\n",
    "          <= q3), 'area_living'] = X.loc[(q2 < X.area_total)\n",
    "            & (X.area_total <= q3), 'area_living'].median()\n",
    "    X.loc[X.area_living.isnull() & (q3 < X.area_total), 'area_living'] = \\\n",
    "        X.loc[q3 < X.area_total, 'area_living'].median()\n",
    "\n",
    "    # fix imputed values that make no sense\n",
    "    X.loc[X.area_total < X.area_kitchen, 'area_kitchen'] = X.area_total \\\n",
    "        * (X.area_kitchen.median() / X.area_total.median())\n",
    "    X.loc[X.area_total < X.area_living, 'area_living'] = X.area_total \\\n",
    "        * (X.area_living.median() / X.area_total.median())\n",
    "\n",
    "    # encode street\n",
    "    X.street, _ = X.street.factorize()\n",
    "\n",
    "    # impute ceiling\n",
    "    X['ceiling'] = X['ceiling'].fillna(X['ceiling'].median())\n",
    "\n",
    "    # impute layout\n",
    "    X['layout'] = X['layout'].fillna(-1.0)\n",
    "\n",
    "    # impute bathrooms with most reasonable combination to assume\n",
    "    X['bathrooms_private'] = X['bathrooms_private'].fillna(1.0)\n",
    "    X['bathrooms_shared'] = X['bathrooms_shared'].fillna(0.0)\n",
    "\n",
    "    # impute windows with mode\n",
    "    # (and also one of two most logical combinations)\n",
    "    X['windows_court'] = X['windows_court'].fillna(1.0)\n",
    "    X['windows_street'] = X['windows_street'].fillna(0.0)\n",
    "\n",
    "    # impute balconies and loggias with modes\n",
    "    X['balconies'] = X['balconies'].fillna(0.0)\n",
    "    X['loggias'] = X['loggias'].fillna(1.0)\n",
    "\n",
    "    # impute condition\n",
    "    X['condition'] = X['condition'].fillna(-1)\n",
    "\n",
    "    # impute phones with mode\n",
    "    X['phones'] = X['phones'].fillna(1.0)\n",
    "\n",
    "    # impute new-ness\n",
    "    X['new'] = X['new'].fillna(0.0)\n",
    "\n",
    "    # impute latitude and longitude with coordinates from Google maps\n",
    "    X['latitude'] = X['latitude'].fillna(55.576675)\n",
    "    X['longitude'] = X['longitude'].fillna(37.4868009)\n",
    "\n",
    "    # impute district\n",
    "    X.loc[X['district'].isna() & (X['latitude'] == 55.595160)\n",
    "          & (X['longitude'] == 37.741109), 'district'] = 5.0\n",
    "    X.loc[X['district'].isna() & (X['latitude'] == 55.576675)\n",
    "          & (X['longitude'] == 37.486801), 'district'] = 11.0\n",
    "    X.loc[X['district'].isna() & (X['latitude'] == 55.921627)\n",
    "          & (X['longitude'] == 37.781578), 'district'] = 2.0\n",
    "    X.loc[X['district'].isna() & (X['latitude'] == 55.583551)\n",
    "          & (X['longitude'] == 37.711356), 'district'] = 5.0\n",
    "    X.loc[X['district'].isna() & (X['latitude'] == 55.932127)\n",
    "          & (X['longitude'] == 37.793705), 'district'] = 2.0\n",
    "    # new category, to denote apartments outside Moscow\n",
    "    X.loc[X['district'].isna(), 'district'] = 12.0\n",
    "\n",
    "    # adjust year built (no effect for the tree-based algorithms)\n",
    "    X['constructed'] = X['constructed'] - X['constructed'].min()\n",
    "    X['constructed'] = X['constructed'].fillna(X['constructed'].median())\n",
    "\n",
    "    # impute material with mode\n",
    "    X['material'] = X['material'].fillna(2.0)\n",
    "\n",
    "    # impute elevator data with mode of all three\n",
    "    X['elevator_without'] = X['elevator_without'].fillna(0.0)\n",
    "    X['elevator_passenger'] = X['elevator_passenger'].fillna(1.0)\n",
    "    X['elevator_service'] = X['elevator_service'].fillna(0.0)\n",
    "    X['elevator_score'] = -1 * X['elevator_without'] + X['elevator_service'\n",
    "            ] + X['elevator_passenger']\n",
    "\n",
    "    # impute parking (mode/median is 1.0)\n",
    "    X['parking'] = X['parking'].fillna(1.0)\n",
    "\n",
    "    # impute garbage chute system (mode is 1)\n",
    "    X['garbage_chute'] = X['garbage_chute'].fillna(1.0)\n",
    "\n",
    "    # impute heating\n",
    "    X['heating'] = X['heating'].fillna(0.0)\n",
    "\n",
    "    # engineer center_distance\n",
    "    X['center_dist'] = list(zip(X.latitude, X.longitude))\n",
    "    X['center_dist'] = X['center_dist'].apply(mhu.dist)\n",
    "    \n",
    "    # engineer distance_loc_[1, 2, 3, 4]\n",
    "    X['distance_loc_1'] = list(zip(X.latitude, X.longitude))\n",
    "    X['distance_loc_1'] = X['distance_loc_1'].apply(mhu.dist_loc_1)\n",
    "    X['distance_loc_2'] = list(zip(X.latitude, X.longitude))\n",
    "    X['distance_loc_2'] = X['distance_loc_2'].apply(mhu.dist_loc_2)\n",
    "    X['distance_loc_3'] = list(zip(X.latitude, X.longitude))\n",
    "    X['distance_loc_3'] = X['distance_loc_3'].apply(mhu.dist_loc_3)\n",
    "    X['distance_loc_4'] = list(zip(X.latitude, X.longitude))\n",
    "    X['distance_loc_4'] = X['distance_loc_4'].apply(mhu.dist_loc_4)\n",
    "    \n",
    "    # engineer direction\n",
    "    X['direction'] = list(zip(X.latitude, X.longitude))\n",
    "    X['direction'] = X['direction'].apply(mhu.direction)\n",
    "    \n",
    "    # Engineer district_center_dist\n",
    "    X['district_center_dist'] = list(zip(X.latitude, X.longitude, X.district))\n",
    "    X['district_center_dist'] = X['district_center_dist'].apply(mhu.dist_dist)\n",
    "\n",
    "    return X\n",
    "\n",
    "def update_train_test(X_train, X_test, all_data, drop_features):\n",
    "    \"\"\"\n",
    "    Replace values in X_train and X_test with corresponding values\n",
    "    in all_data\n",
    "    \"\"\"\n",
    "    # add new features train and test datasets\n",
    "    new_features = list(set(all_data.columns) - set(X_train.columns))\n",
    "    X_train[new_features] = np.NaN\n",
    "    X_test[new_features] = np.NaN\n",
    "    \n",
    "    # drop any original features\n",
    "    all_data.drop(labels=drop_features, axis=1, inplace=True)\n",
    "    X_train.drop(labels=drop_features, axis=1, inplace=True)\n",
    "    X_test.drop(labels=drop_features, axis=1, inplace=True)\n",
    "\n",
    "    X_train.set_index('id', inplace=True)\n",
    "    X_test.set_index('id', inplace=True)\n",
    "    X_train.update(all_data.set_index('id'))\n",
    "    X_test.update(all_data.set_index('id'))\n",
    "    X_train.reset_index()\n",
    "    X_test.reset_index()\n",
    "\n",
    "def plot_feature_importances(model, model_name, cols):\n",
    "    \"\"\"\n",
    "    Analyse feature importance using mean decrease in impurity (MDI)\n",
    "    \"\"\"\n",
    "    importances = pd.Series(model.feature_importances_, index=cols)\n",
    "    fig, ax = plt.subplots(figsize=(18, 15))\n",
    "    importances.plot.bar(ax=ax)\n",
    "    ax.set_title(f\"{model_name} - MDI / Gini Importance\")\n",
    "    ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "def plot_map(data, column='price', title='', ax=None, s=5, a=0.75, q_lo=0.0, q_hi=0.9, cmap='autumn'):\n",
    "    \"\"\"\n",
    "    Plot 'column' in 'data' on Moscow map backdrop\n",
    "    \"\"\"\n",
    "    data = data[['latitude', 'longitude', column]].sort_values(by=column, ascending=True)\n",
    "    if not title:\n",
    "        title = f\"{column.title()} by location\"\n",
    "    backdrop = plt.imread('../data/moscow.png')\n",
    "    backdrop = np.einsum('hwc, c -> hw', backdrop, [0, 1, 0, 0]) ** 2\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(12, 8), dpi=100)\n",
    "        ax = plt.gca()\n",
    "    discrete = data[column].nunique() <= 20\n",
    "    if not discrete:\n",
    "        lo, hi = data[column].quantile([q_lo, q_hi])\n",
    "        hue_norm = plt.Normalize(lo, hi)\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(lo, hi))\n",
    "        sm.set_array([])\n",
    "    else:\n",
    "        hue_norm = None \n",
    "    ax.imshow(backdrop, alpha=0.5, extent=[37, 38, 55.5, 56], aspect='auto', cmap='bone', norm=plt.Normalize(0.0, 2))\n",
    "    sns.scatterplot(x='longitude', y='latitude', hue=data[column].tolist(), ax=ax, s=s, alpha=a, palette=cmap, linewidth=0, hue_norm=hue_norm, data=data)\n",
    "    ax.set_xlim(37, 38)    # min/max longitude of image \n",
    "    ax.set_ylim(55.5, 56)  # min/max latitude of image\n",
    "    if not discrete:\n",
    "        ax.legend().remove()\n",
    "        ax.figure.colorbar(sm)\n",
    "    ax.set_title(title)\n",
    "    return ax, hue_norm\n",
    "\n",
    "def plot_predictions_on_map(X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot predicted prices on Moscow map backdrop\n",
    "    \"\"\"\n",
    "    data = X_test.copy()\n",
    "    data[\"price\"] = y_test\n",
    "    plot_map(data, title=\"Predicted price by location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb66de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for storing all models' predictions\n",
    "MODELS_NUM = 5\n",
    "preds = [None] * MODELS_NUM\n",
    "\n",
    "int_cols = [\n",
    "    'seller', 'floor', 'rooms', 'layout', 'bathrooms_shared',\n",
    "    'bathrooms_private', 'windows_court', 'windows_street', 'balconies',\n",
    "    'loggias', 'condition', 'phones', 'new', 'district', 'constructed',\n",
    "    'material', 'stories', 'parking', 'garbage_chute', 'heating',\n",
    "    'elevator_score', 'street'\n",
    "]\n",
    "category_cols = ['seller', 'layout', 'district', 'heating', 'material',\n",
    "                 'street', 'condition']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b571cde",
   "metadata": {},
   "source": [
    "## Model 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca4ad9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test datasets\n",
    "unused_features = ['address', 'building_id']\n",
    "X_train = data.drop(unused_features + ['price'], axis=1)\n",
    "y_train = np.log1p(data.price)\n",
    "X_test = data_test.drop(unused_features, axis=1)\n",
    "\n",
    "# merge train and test for combined data processing\n",
    "X = X_train.append(X_test, ignore_index=True)\n",
    "X = data_wrangling_optimal(X)\n",
    "\n",
    "# update train and test sets\n",
    "drop_features = ['elevator_without', 'elevator_service', 'elevator_passenger']\n",
    "update_train_test(X_train, X_test, all_data=X, drop_features=drop_features)\n",
    "\n",
    "# convert columns to proper dtype\n",
    "X_train[int_cols] = X_train[int_cols].astype('int32')\n",
    "X_test[int_cols] = X_test[int_cols].astype('int32')\n",
    "\n",
    "for col in category_cols:\n",
    "    X_train[col] = pd.Categorical(X_train[col])\n",
    "    X_test[col] = pd.Categorical(X_test[col])\n",
    "cat_idx = [X_train.columns.get_loc(col) for col in category_cols]\n",
    "\n",
    "X_train_model0 = X_train.copy()\n",
    "X_test_model0 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6be0e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matey\\miniconda3\\envs\\mlcourse\\lib\\site-packages\\lightgbm\\basic.py:1222: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning('{0} keyword has been found in `params` and will be ignored.\\n'\n",
      "C:\\Users\\matey\\miniconda3\\envs\\mlcourse\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_feature in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    }
   ],
   "source": [
    "model0 = LGBMRegressor(\n",
    "    colsample_bytree=0.8346198485840488,\n",
    "    learning_rate=0.038772388344039496,\n",
    "    max_bin=1023,\n",
    "    min_child_samples=17,\n",
    "    n_estimators=23284,\n",
    "    num_leaves=38,\n",
    "    reg_alpha=0.003539069387982963,\n",
    "    reg_lambda=7.774356662353698,\n",
    "    verbose=-1,\n",
    "    categorical_feature=cat_idx,\n",
    "    random_state=SEED\n",
    ")\n",
    "model0.fit(X_train_model0, y_train)\n",
    "preds[0] = np.floor(np.expm1(model0.predict(X_test_model0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591f1cf",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ed210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test datasets\n",
    "unused_features = ['address', 'building_id']\n",
    "X_train = data.drop(unused_features + ['price'], axis=1)\n",
    "y_train = np.log1p(data.price)\n",
    "X_test = data_test.drop(unused_features, axis=1)\n",
    "\n",
    "# merge train and test for combined data processing\n",
    "X = X_train.append(X_test, ignore_index=True)\n",
    "X = data_wrangling_optimal(X)\n",
    "\n",
    "##############################################################################\n",
    "# change what is different in data processing from best LGBM pipeline\n",
    "\n",
    "# increase the value of underground parking\n",
    "X.loc[X['parking'] == 1.0, 'parking'] = 4.0\n",
    "# boolean heating - central or not\n",
    "X['heating'] = (X['heating'] == 0.0)\n",
    "# engineer percieved area\n",
    "X['percieved_area'] = 2 * X['area_living'] + X['area_kitchen'] \\\n",
    "    + 0.5 * (X['area_total'] - X['area_living'] - X['area_kitchen'])\n",
    "# engineer brightness\n",
    "X['brightness'] = 2*X['balconies'] + X['loggias']\n",
    "# engineer ratio living/total area\n",
    "X['spaciousness'] = ( X.area_living / X.rooms).round(decimals=3)\n",
    "\n",
    "# engineer price per square meter per district\n",
    "X_train['sq_meter_price'] = data['price']/X_train['area_total']\n",
    "X_train['sq_meter_price'] = X_train.groupby('district')['sq_meter_price'] \\\n",
    "    .transform(lambda x: round(x.median(), 2))\n",
    "dist_medians = X_train[['district', 'sq_meter_price']].drop_duplicates()\n",
    "d = dist_medians.set_index('district').T.to_dict('records').pop()\n",
    "X_test['sq_meter_price'] = np.nan\n",
    "X_test['sq_meter_price'] = X_test['district'].apply(lambda x: d.get(x))\n",
    "# imputing the median for the cheapest district\n",
    "# for apartments outside Moscow\n",
    "X_test['sq_meter_price'] = X_test['sq_meter_price'].fillna(d.get(10.0))\n",
    "# engineer price based on square meter price (target encoding)\n",
    "X_train['price_enc'] = X_train.sq_meter_price * X_train.area_total\n",
    "X_test['price_enc'] = X_test.sq_meter_price * X_test.area_total\n",
    "\n",
    "# update train and test sets\n",
    "drop_features = ['elevator_without', 'elevator_service', 'elevator_passenger',\n",
    "                 'area_living', 'area_kitchen', 'district', 'rooms',\n",
    "                 'latitude', 'longitude', 'area_total', 'ceiling']\n",
    "update_train_test(X_train, X_test, all_data=X, drop_features=drop_features)\n",
    "##############################################################################\n",
    "\n",
    "# convert columns to proper dtype\n",
    "ints = int_cols.copy()\n",
    "ints.remove('district')\n",
    "ints.remove('rooms')\n",
    "X_train[ints] = X_train[ints].astype('int32')\n",
    "X_test[ints] = X_test[ints].astype('int32')\n",
    "\n",
    "cats = category_cols.copy()\n",
    "cats.remove('district')\n",
    "for col in cats:\n",
    "    X_train[col] = pd.Categorical(X_train[col])\n",
    "    X_test[col] = pd.Categorical(X_test[col])\n",
    "cat_idx = [X_train.columns.get_loc(col) for col in cats]\n",
    "\n",
    "X_train_model1 = X_train.copy()\n",
    "X_test_model1 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd56ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LGBMRegressor(\n",
    "    colsample_bytree=0.8346198485840488,\n",
    "    learning_rate=0.038772388344039496,\n",
    "    max_bin=1023,\n",
    "    min_child_samples=17,\n",
    "    n_estimators=23284,\n",
    "    num_leaves=38,\n",
    "    reg_alpha=0.003539069387982963,\n",
    "    reg_lambda=7.774356662353698,\n",
    "    verbose=-1,\n",
    "    categorical_feature=cat_idx,\n",
    "    random_state=SEED\n",
    ")\n",
    "model1.fit(X_train_model1, y_train)\n",
    "preds[1] = np.floor(np.expm1(model1.predict(X_test_model1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac5ffb0e-342e-4f3d-8a7f-469f91763497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.976561085472111, 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check correlation between predictions\n",
    "pearsonr(preds[0], preds[1]) # we mainly care about first number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e07f8d",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac68183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test datasets\n",
    "unused_features = ['address', 'building_id']\n",
    "X_train = data.drop(unused_features + ['price'], axis=1)\n",
    "y_train = np.log1p(data.price)\n",
    "X_test = data_test.drop(unused_features, axis=1)\n",
    "\n",
    "# merge train and test for combined data processing\n",
    "X = X_train.append(X_test, ignore_index=True)\n",
    "X = data_wrangling_optimal(X)\n",
    "\n",
    "##############################################################################\n",
    "# change what is different in data processing from best LGBM pipeline\n",
    "\n",
    "# OHE categorical features\n",
    "ints = int_cols.copy()\n",
    "ints.remove('street')\n",
    "cats = category_cols.copy()\n",
    "cats.remove('street')\n",
    "X = pd.get_dummies(X, columns=cats)\n",
    "\n",
    "# update train and test sets\n",
    "drop_features = ['elevator_without', 'elevator_service', 'elevator_passenger',\n",
    "                 'street']\n",
    "update_train_test(X_train, X_test, all_data=X, drop_features=drop_features)\n",
    "X_train.drop(cats, axis=1, inplace=True)\n",
    "X_test.drop(cats, axis=1, inplace=True)\n",
    "##############################################################################\n",
    "\n",
    "X_train_model2 = X_train.copy()\n",
    "X_test_model2 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1486910",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestRegressor(\n",
    "    max_features=0.6913722322623973,\n",
    "    max_leaf_nodes=23284,\n",
    "    n_estimators=667,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model2.fit(X_train_model2, y_train)\n",
    "preds[2] = np.floor(np.expm1(model2.predict(X_test_model2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "031e5e5d-d855-4831-93ab-804bb8c0a56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9604991464923318, 0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check correlation between predictions\n",
    "pearsonr(preds[0], preds[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24b69d",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42852c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test datasets\n",
    "unused_features = ['address', 'building_id']\n",
    "X_train = data.drop(unused_features + ['price'], axis=1)\n",
    "y_train = np.log1p(data.price)\n",
    "X_test = data_test.drop(unused_features, axis=1)\n",
    "\n",
    "# merge train and test for combined data processing\n",
    "X = X_train.append(X_test, ignore_index=True)\n",
    "X = data_wrangling_optimal(X)\n",
    "\n",
    "# update train and test sets\n",
    "drop_features = ['elevator_without', 'elevator_service', 'elevator_passenger']\n",
    "update_train_test(X_train, X_test, all_data=X, drop_features=drop_features)\n",
    "\n",
    "# convert columns to proper dtype\n",
    "X_train[int_cols] = X_train[int_cols].astype('int32')\n",
    "X_test[int_cols] = X_test[int_cols].astype('int32')\n",
    "\n",
    "for col in category_cols:\n",
    "    X_train[col] = pd.Categorical(X_train[col])\n",
    "    X_test[col] = pd.Categorical(X_test[col])\n",
    "cat_idx = [X_train.columns.get_loc(col) for col in category_cols]\n",
    "\n",
    "X_train_model3 = X_train.copy()\n",
    "X_test_model3 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7a8102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = CatBoostRegressor(\n",
    "    n_estimators=1889,\n",
    "    learning_rate=0.2,\n",
    "    thread_count=-1,\n",
    "    depth=6,\n",
    "    silent=True,\n",
    "    bagging_temperature=0.2,\n",
    "    early_stopping_rounds=63,\n",
    "    cat_features=cat_idx,\n",
    "    allow_writing_files=False,\n",
    "    random_seed=SEED\n",
    ")\n",
    "model3.fit(X_train_model3, y_train)\n",
    "preds[3] = np.floor(np.expm1(model3.predict(X_test_model3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9589b426-c2a5-41fe-9c32-2e1c3ae8dcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9516869986471448, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check correlation between predictions\n",
    "pearsonr(preds[0], preds[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a04ca7",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbbef92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = GradientBoostingRegressor(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=15,\n",
    "    min_samples_split=10, \n",
    "    loss='huber',\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "model4.fit(X_train_model0, y_train)\n",
    "preds[4] = np.floor(np.expm1(model4.predict(X_test_model0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c69f52bc-ca23-47a8-ac6a-d13e7589aa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9764273067341966, 0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(preds[0], preds[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7859d",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2400c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for easier calculation and storing of results\n",
    "preds_df = pd.DataFrame(preds)\n",
    "preds_df = preds_df.transpose()\n",
    "preds_df.columns = ['lgbm_best', 'lgbm_mod', 'random_forest', 'catboost', 'gradient_boost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9009f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple mean\n",
    "preds_df[\"average\"] = preds_df.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63754b-61c3-40ab-83bd-9c3a7cec3a29",
   "metadata": {},
   "source": [
    "The simple mean of the 5 used models is corresponding to our best result on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec504de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "NFOLDS = 5 # set number of folds for out-of-fold prediction\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=NFOLDS,\n",
    "    shuffle=True,\n",
    "    random_state=SEED\n",
    ") # K-Folds cross-validator\n",
    "\n",
    "def get_oof(clf, x_train, y_train, x_test, outlier_limit=20):\n",
    "    \"\"\"\n",
    "    Popular function on Kaggle.\n",
    "    \n",
    "    Trains a classifier on 4/5 of the training data and\n",
    "    predicts the rest (1/5). This procedure is repeated for all 5 folds,\n",
    "    thus we have predictions for all training set. This prediction is one\n",
    "    column of meta-data, later on used as a feature column by a meta-algorithm.\n",
    "    We predict the test part and average predictions across all 5 models.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    clf -- classifier\n",
    "    x_train -- 4/5 of training data\n",
    "    y_train -- corresponding labels\n",
    "    x_test -- all test data\n",
    "    outlier_limit -- outlier limit for rounded price log\n",
    "    \n",
    "    \"\"\"\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "    \n",
    "    y_strat = y_train.round()\n",
    "    # bundle all high outliers in one class\n",
    "    y_strat[y_strat > outlier_limit] = outlier_limit + 1\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(x_train, y_strat)):\n",
    "        x_tr = x_train.iloc[train_index]\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        x_te = x_train.iloc[test_index]\n",
    "\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        \n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "379a4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matey\\miniconda3\\envs\\mlcourse\\lib\\site-packages\\lightgbm\\basic.py:1222: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning('{0} keyword has been found in `params` and will be ignored.\\n'\n",
      "C:\\Users\\matey\\miniconda3\\envs\\mlcourse\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_feature in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "C:\\Users\\matey\\miniconda3\\envs\\mlcourse\\lib\\site-packages\\lightgbm\\basic.py:1222: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
      "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
      "  _log_warning('{0} keyword has been found in `params` and will be ignored.\\n'\n",
      "C:\\Users\\matey\\miniconda3\\envs\\mlcourse\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_feature in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    }
   ],
   "source": [
    "m0_oof_train, m0_oof_test = get_oof(model0, X_train_model0, y_train, X_test_model0)\n",
    "m1_oof_train, m1_oof_test = get_oof(model1, X_train_model1, y_train, X_test_model1)\n",
    "m2_oof_train, m2_oof_test = get_oof(model2, X_train_model2, y_train, X_test_model2)\n",
    "m3_oof_train, m3_oof_test = get_oof(model3, X_train_model3, y_train, X_test_model3)\n",
    "m4_oof_train, m4_oof_test = get_oof(model4, X_train_model0, y_train, X_test_model0)\n",
    "\n",
    "# First-level output as new features\n",
    "\n",
    "x_train = np.concatenate((\n",
    "    m0_oof_train,\n",
    "    m1_oof_train,\n",
    "    m2_oof_train,\n",
    "    m3_oof_train,\n",
    "    m4_oof_train\n",
    "), axis=1)\n",
    "\n",
    "x_test = np.concatenate((\n",
    "    m0_oof_test,\n",
    "    m1_oof_test,\n",
    "    m2_oof_test,\n",
    "    m3_oof_test,\n",
    "    m4_oof_test\n",
    "), axis=1)\n",
    "\n",
    "meta_features = ['lgbm_best', 'lgbm_mod', 'random_forest', 'catboost', 'gradient_boost']\n",
    "x_train_meta = pd.DataFrame(data=x_train, columns=meta_features)\n",
    "x_test_meta = pd.DataFrame(data=x_test, columns=meta_features)\n",
    "\n",
    "META_MODEL = ExtraTreesRegressor(max_features=0.6679422374183948, max_leaf_nodes=161,\n",
    "                    n_estimators=154, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "META_MODEL.fit(x_train_meta, y_train)\n",
    "preds_df[\"true_stacking\"] = np.floor(np.expm1(META_MODEL.predict(x_test_meta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd12f95-e5ec-437c-973f-ec888e9e983a",
   "metadata": {},
   "source": [
    "# Storing and retrieving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ef4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dataframe with predictions\n",
    "preds_df.to_csv('predictions.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7237725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve predictions from csv\n",
    "preds_df = pd.read_csv('predictions.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584699f4-d48a-4ef3-acd2-7af504fccb82",
   "metadata": {},
   "source": [
    "# Final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f220e-8be1-439d-bfe5-4aca4e30e1aa",
   "metadata": {},
   "source": [
    "This is our best scoring submission dataframe. For reproducibility issues, please see note at the end of the long notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3590445d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9937 predictions\n"
     ]
    }
   ],
   "source": [
    "# Construct submission dataframe for simple mean predictions\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test.id\n",
    "submission['price_prediction'] = preds_df[\"average\"].values\n",
    "print(f'Generated {len(submission)} predictions')\n",
    "\n",
    "# Export submission to csv with headers\n",
    "submission.to_csv('solution_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2388ac-5ba3-4f48-9660-d10a007d6a3f",
   "metadata": {},
   "source": [
    "This is the dataframe for our second chosen solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20ff9871-b891-45f1-a5cd-31e002de15f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 9937 predictions\n"
     ]
    }
   ],
   "source": [
    "# Construct submission dataframe for ensemble predictions\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test.id\n",
    "submission['price_prediction'] = preds_df[\"true_stacking\"].values\n",
    "print(f'Generated {len(submission)} predictions')\n",
    "\n",
    "# Export submission to csv with headers\n",
    "submission.to_csv('solution_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
